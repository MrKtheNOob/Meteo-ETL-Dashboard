# Pipeline ETL M√©t√©o et Entrep√¥t de Donn√©es

## Introduction

Ce projet a mis en ≈ìuvre un **pipeline ETL** pour collecter, traiter et stocker des donn√©es m√©t√©orologiques de l'API WeatherAPI.com dans un entrep√¥t de donn√©es. Un tableau de bord a √©galement √©t√© d√©velopp√© pour la visualisation et la validation des donn√©es.

-----

## Approche et Architecture

Un **syst√®me ETL personnalis√© bas√© sur Python** a √©t√© choisi pour sa flexibilit√© et son contr√¥le, permettant une gestion pr√©cise de chaque √©tape du flux de donn√©es. Le pipeline a √©t√© con√ßu pour :

  * **R√©cup√©rer s√©quentiellement** les donn√©es via l'API WeatherAPI.com.
  * **Traiter et nettoyer** les donn√©es brutes.
  * **Ins√©rer** les donn√©es dans une base de donn√©es source relationnelle, puis dans un entrep√¥t de donn√©es.
  * **G√©rer les limitations de l'API** et les contraintes de ressources (CPU/RAM) via des pauses (`time.sleep()`).

-----

## Conception des Sch√©mas de Base de Donn√©es

### Sch√©ma de la Base de Donn√©es Source

Un **sch√©ma relationnel** a √©t√© cr√©√© pour la base source, optimis√© pour les **op√©rations transactionnelles** et l'int√©grit√© des donn√©es brutes.

### Sch√©ma de l'Entrep√¥t de Donn√©es (Sch√©ma en √âtoile)

Un **sch√©ma en √©toile** a √©t√© impl√©ment√© pour l'entrep√¥t de donn√©es, optimis√© pour l'**analyse et le reporting**. Il comprend une **table de faits centrale** (`FaitDonneesMeteo`) et des **tables de dimensions** (`DimLieux`, `DimConditionsMeteo`, `DimTemps`), facilitant l'exploration et l'analyse des donn√©es.

-----

## Processus ETL et Transformations

Les donn√©es extraites de l'API ont √©t√© transform√©es pour passer d'un format transactionnel √† une structure d√©normalis√©e adapt√©e au sch√©ma en √©toile. Cela a impliqu√© :

  * **Parsing et validation des JSON** via Pydantic.
  * **Mappage des attributs** aux tables de faits et de dimensions.
  * **D√©composition des horodatages** pour la dimension temps.
  * **Insertion des donn√©es** via `pymysql`, avec gestion explicite des transactions pour la coh√©rence.

-----

## API de Donn√©es M√©t√©o

Une API Flask a √©t√© d√©velopp√©e pour exposer les donn√©es de l'entrep√¥t et g√©rer le processus ETL :

  * `/trigger-etl` (GET) : D√©clenche le processus ETL en arri√®re-plan.
  * `/etl-status` (GET) : Donne le statut de l'ETL.
  * `/api/weather/all` (GET) : R√©cup√®re toutes les donn√©es m√©t√©o.
  * `/api/weather/locations` (GET) : Liste les villes disponibles.
  * `/` et `/<path:filename>` (GET) : Servent le frontend et les fichiers statiques.

**Flask-Caching** a √©t√© utilis√© pour optimiser les r√©ponses, et le traitement ETL s'ex√©cute dans un **thread s√©par√©** pour √©viter de bloquer l'API.

-----

## Tableau de Bord et Validation

Un tableau de bord interactif a √©t√© d√©velopp√© avec **React** pour interroger l'API Flask, permettant une exploration dynamique et une validation des donn√©es.

üîó Tableau de bord en ligne : [https://meteo-etl-dashboard.onrender.com/](https://www.google.com/search?q=https://meteo-etl-dashboard.onrender.com/)

-----

## Int√©gration Additionnelle (Airflow)

Apr√®s la finalisation du pipeline Python, un prototype local **Airflow** a √©t√© int√©gr√© pour d√©montrer la capacit√© √† orchestrer des flux de donn√©es complexes en entreprise. L'**entr√©e principale de l'ETL est `ETL.py`** et l'**API est servie par `api.py`**.

-----

## Configuration de Airflow

Le r√©pertoire Airflow est structur√© comme suit :

```
. ‚îú‚îÄ‚îÄ airflow
‚îÇ ‚îú‚îÄ‚îÄ airflow.cfg
‚îÇ ‚îú‚îÄ‚îÄ airflow.db
‚îÇ ‚îú‚îÄ‚îÄ dags
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ weather_etl_dag.py
‚îÇ ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ ‚îî‚îÄ‚îÄ dockerfile
```

### ‚úÖ Pr√©requis

  * **Docker** et **Docker Compose** install√©s.
  * Connaissances de base du terminal.

### Contenu du Dossier Expliqu√©

  * `airflow.cfg` : Fichier de configuration Airflow.
  * `airflow.db` : Base de donn√©es SQLite locale pour les m√©tadonn√©es Airflow.
  * `dags/` : Contient vos DAGs (ex: `weather_etl_dag.py`).
  * `docker-compose.yml` : D√©finit les services Docker pour Airflow.
  * `dockerfile` : Permet de cr√©er une image Docker personnalis√©e (ex: pour installer des packages).

### Cr√©er l'Image Docker Airflow

Dans le r√©pertoire `airflow` :

```bash
cd airflow
docker build -t custom-airflow:latest -f dockerfile .
```

-----

### ‚öôÔ∏è V√©rifier / Modifier `docker-compose.yml`

Assurez-vous que `docker-compose.yml` utilise l'image personnalis√©e que vous venez de cr√©er :

```yaml
version: '3'
services:
  airflow:
    image: custom-airflow:latest
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./airflow.db:/opt/airflow/airflow.db
    ports:
      - "8080:8080"
    command: webserver
```

-----

### D√©marrer Airflow

```bash
docker-compose up
```

Ceci ex√©cute le serveur web Airflow √† l'adresse [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080).

-----

### Initialiser la Base de Donn√©es (uniquement lors de la premi√®re ex√©cution)

Si c'est votre premi√®re ex√©cution :

```bash
docker-compose run airflow airflow db init
```

Puis red√©marrez :

```bash
docker-compose up
```

-----

### Acc√©der √† l'Interface Utilisateur d'Airflow

Ouvrez [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080).
Identifiants par d√©faut : `airflow` / `airflow` (si inchang√©).

-----

### V√©rifiez votre DAG

Dans l'interface utilisateur, vous devriez voir `weather_etl_dag.py` list√©. Activez-le et d√©clenchez-le si n√©cessaire.

-----

### Arr√™ter Airflow

```bash
docker-compose down
```

-----

### üí¨ Remarques

  * L'utilisation de **SQLite** est id√©ale pour les tests locaux. Pour la production, utilisez **Postgres** ou **MySQL**.
  * Votre `airflow.db` est mont√© en volume, garantissant la **persistance des donn√©es** apr√®s le red√©marrage du conteneur.
  * Personnalisez le `dockerfile` pour installer des packages Python ou d'autres outils n√©cessaires (ex: `RUN pip install requests`).

-----

### ‚úÖ R√©sum√©

1Ô∏è‚É£ **Construire votre image** üß±
2Ô∏è‚É£ **V√©rifier** que `docker-compose.yml` pointe vers elle üéØ
3Ô∏è‚É£ **Ex√©cuter** `docker-compose up` üöÄ
4Ô∏è‚É£ **Acc√©der** √† l'interface utilisateur √† `localhost:8080` üåê
